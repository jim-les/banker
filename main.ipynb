{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence Label Unnamed: 2  \\\n",
      "0                  \" or pg_sleep  (  __TIME__  )  --     1        NaN   \n",
      "1  create user name identified by pass123 tempora...   NaN          1   \n",
      "2   AND 1  =  utl_inaddr.get_host_address   (    ...     1        NaN   \n",
      "3   select * from users where id  =  '1' or @ @1 ...     1        NaN   \n",
      "4   select * from users where id  =  1 or 1#\"  ( ...     1        NaN   \n",
      "\n",
      "   Unnamed: 3  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/SQLIV3.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract sentences and labels\n",
    "sentences = df['Sentence']\n",
    "labels = df['Label']\n",
    "\n",
    "# Preprocess the data\n",
    "max_words = 1000  # Maximum number of words in your vocabulary\n",
    "max_len = 50  # M\n",
    "\n",
    "sentences = sentences.fillna('').astype(str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "X = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build an advanced LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 572ms/step - accuracy: 0.0000e+00 - loss: -7681.2988 - val_accuracy: 2.0214e-04 - val_loss: -59201.8906\n",
      "Epoch 2/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 432ms/step - accuracy: 0.0000e+00 - loss: -94212.8984 - val_accuracy: 2.0214e-04 - val_loss: -229004.1562\n",
      "Epoch 3/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 443ms/step - accuracy: 0.0000e+00 - loss: -289796.9375 - val_accuracy: 2.0214e-04 - val_loss: -501445.4062\n",
      "Epoch 4/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 415ms/step - accuracy: 0.0000e+00 - loss: -587413.8750 - val_accuracy: 2.0214e-04 - val_loss: -865840.5000\n",
      "Epoch 5/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 417ms/step - accuracy: 0.0000e+00 - loss: -970798.7500 - val_accuracy: 2.0214e-04 - val_loss: -1314547.1250\n",
      "Epoch 6/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 421ms/step - accuracy: 0.0000e+00 - loss: -1449100.8750 - val_accuracy: 2.0214e-04 - val_loss: -1840588.7500\n",
      "Epoch 7/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 424ms/step - accuracy: 0.0000e+00 - loss: -1983562.3750 - val_accuracy: 2.0214e-04 - val_loss: -2436643.2500\n",
      "Epoch 8/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 427ms/step - accuracy: 0.0000e+00 - loss: -2600684.5000 - val_accuracy: 2.0214e-04 - val_loss: -3098396.2500\n",
      "Epoch 9/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 432ms/step - accuracy: 0.0000e+00 - loss: -3284555.7500 - val_accuracy: 2.0214e-04 - val_loss: -3824624.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 442ms/step - accuracy: 0.0000e+00 - loss: -4021701.7500 - val_accuracy: 2.0214e-04 - val_loss: -4609755.5000\n",
      "Epoch 11/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 451ms/step - accuracy: 0.0000e+00 - loss: -4821219.5000 - val_accuracy: 2.0214e-04 - val_loss: -5453447.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 437ms/step - accuracy: 0.0000e+00 - loss: -5677470.0000 - val_accuracy: 2.0214e-04 - val_loss: -6351672.5000\n",
      "Epoch 13/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 464ms/step - accuracy: 0.0000e+00 - loss: -6592868.5000 - val_accuracy: 2.0214e-04 - val_loss: -7304881.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 434ms/step - accuracy: 0.0000e+00 - loss: -7563109.5000 - val_accuracy: 2.0214e-04 - val_loss: -8307774.5000\n",
      "Epoch 15/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 444ms/step - accuracy: 0.0000e+00 - loss: -8551421.0000 - val_accuracy: 2.0214e-04 - val_loss: -9364120.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 433ms/step - accuracy: 0.0000e+00 - loss: -9663465.0000 - val_accuracy: 2.0214e-04 - val_loss: -10473959.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 461ms/step - accuracy: 0.0000e+00 - loss: -10757338.0000 - val_accuracy: 2.0214e-04 - val_loss: -11629505.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 452ms/step - accuracy: 0.0000e+00 - loss: -11959241.0000 - val_accuracy: 2.0214e-04 - val_loss: -12838747.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 443ms/step - accuracy: 0.0000e+00 - loss: -13167506.0000 - val_accuracy: 2.0214e-04 - val_loss: -14097884.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 435ms/step - accuracy: 0.0000e+00 - loss: -14430435.0000 - val_accuracy: 2.0214e-04 - val_loss: -15404615.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x262cbdc1670>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Implement early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 108ms/step - accuracy: 0.0000e+00 - loss: -15417264.0000\n",
      "Test Loss: -15420262.0000, Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_attack_details(sentence, predicted_label, actual_label):\n",
    "    with open('attack_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"Sentence: {sentence}\\n\")\n",
    "        log_file.write(f\"Predicted Label: {predicted_label}\\n\")\n",
    "        log_file.write(f\"Actual Label: {actual_label}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have a list of sentences to classify for feedback\n",
    "sentences_to_classify = [\"Suspicious SQL injection attempt\", \"Normal user query\", \"Another attack example\"]\n",
    "\n",
    "# Classify the sentences and log details for attacks\n",
    "for sentence in sentences_to_classify:\n",
    "    # Preprocess the sentence and tokenize it\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "    # Predict the label for the sentence\n",
    "    predicted_label = model.predict(padded_sequence)\n",
    "\n",
    "    # Assuming you have a threshold for classifying attacks\n",
    "    threshold = 0.5\n",
    "    if predicted_label > threshold:\n",
    "        # Log the attack details for feedback\n",
    "        log_attack_details(sentence, predicted_label, \"Attack\")\n",
    "    else:\n",
    "        log_attack_details(sentence, predicted_label, \"Normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Load your best classification model (replace 'best_model.h5' with your model's file)\n",
    "best_model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([sentence])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# sequence = tokenizer.text_to_sequences([sentence])\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m padded_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m(sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_len)\n\u001b[0;32m     12\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(padded_sequence)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_label)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Create an instance of Tokenizer\n",
    "max_words = 1000  # Maximum number of words in your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "# Rest of the code\n",
    "sentence = \"SQL injection attempt\"\n",
    "sequence = tokenizer.texts_to_sequences([sentence])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "predicted_label = best_model.predict(padded_sequence)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
